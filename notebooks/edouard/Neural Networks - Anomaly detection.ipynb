{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks for anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Debug mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For relative imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading observation A1\n",
      "Loading observation A2\n",
      "Loading observation B1\n",
      "Loading observation B2\n",
      "Loading observation C1\n",
      "Loading observation C2\n",
      "Loading observation D1\n",
      "Loading observation D2\n",
      "Loading observation E1\n",
      "Loading observation E2\n",
      "Loading observation F1\n",
      "Loading observation F2\n",
      "Loading observation G1\n",
      "Loading observation G2\n",
      "Loading observation H1\n",
      "Loading observation H2\n",
      "Loading observation B3\n",
      "Loading observation B4\n",
      "Loading observation F3\n",
      "Loading observation F4\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import mpld3\n",
    "from interval import Interval\n",
    "from observation import Observation\n",
    "from constants import *\n",
    "from datetime import timedelta\n",
    "from tools import sequence_to_interval\n",
    "plt.style.use('ggplot')\n",
    "mpld3.enable_notebook()\n",
    "\n",
    "PATH = \"../../../Data/GMPP_IRSDI/\"\n",
    "reactor_sites = [site+tranche for site in [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"] for tranche in [\"1\",\"2\"]] + [\"B3\",\"B4\",\"F3\",\"F4\"]\n",
    "\n",
    "suffixes = [\n",
    "\"DEB1-1\",\"DEB1-2\",\"DEB1-3\",#,\"DEB1-4\", # Débit de fuite au joint 1 (Gamme Large)\n",
    "# \"DEB2-1\",\"DEB2-2\",\"DEB2-3\",\"DEB2-4\", # Débit de fuite au joint 1 (Gamme Étroite)\n",
    "# \"DEB3-1\",\"DEB3-2\",\"DEB3-3\",\"DEB3-4\",\"DEB3-5\", # Débit d'injection au joint\n",
    " \"PUI-\",  # Puissance thermique moyenne\n",
    " \"PRE-\"  # Pression\n",
    "# \"TEM1-\", # Température ligne d'injection aux joints (en * Celsius) ### A rapprocher de DEB3\n",
    "# \"TEM2-\", # Température fuites joint 1\n",
    "# \"TEM3-1\",\"TEM3-2\",\"TEM3-3\",\"TEM3-4\",# Température eau joint 1 - 051PO ### A rapprocher de DEB1 DEB2\n",
    "# \"VIT-1\",\"VIT-2\",\"VIT-3\",\"VIT-4\"# Vitesse de rotation\n",
    "] \n",
    "\n",
    "obs_dict = {}\n",
    "\n",
    "for reactor_site in reactor_sites:\n",
    "    print(\"Loading observation \"+reactor_site)\n",
    "    obs_dict[reactor_site] = Observation(PATH,reactor_site,suffixes,verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Re- exectute that everytime you have a \n",
    "# TypeError: super(type, obj): obj must be an instance or subtype of type\n",
    "from scale import *\n",
    "from feature import *\n",
    "from tools import *\n",
    "from score import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring PCA to find anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'half_window' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-390e1db5b446>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"A1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_regime_intervals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_between\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"A1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_concatenated_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeb1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhalf_window\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhalf_window\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhalf_window\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhalf_window\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpca_on_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'half_window' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df = obs_dict[\"A1\"].low_regime_intervals.split_between(obs_dict[\"A1\"].full_concatenated_df, time=timedelta(days=3))[-1][deb1[0]]\n",
    "data = df.values\n",
    "data = np.array([data[t - half_window:t + half_window] for t in range(half_window, len(df) - half_window)])\n",
    "\n",
    "def pca_on_data(n_components, data):\n",
    "    pca = PCA(n_components)\n",
    "    rolling_size=40\n",
    "    half_window = int(rolling_size / 2)\n",
    "    pca.fit(data)\n",
    "    return pca\n",
    "\n",
    "pca = pca_on_data(5, data)\n",
    "score_5 = np.sum(np.power(pca.inverse_transform(pca.transform(data)) - data,2),axis=1)\n",
    "pca = pca_on_data(15, data)\n",
    "score_15 = np.sum(np.power(pca.inverse_transform(pca.transform(data)) - data,2),axis=1)\n",
    "index = df.index[half_window:len(df) - half_window]\n",
    "output_dataframe = np.concatenate([score_5,score_15]).reshape(-1,2)\n",
    "dataframe = pd.DataFrame(index=index, data=output_dataframe, columns=[\"score_5\",\"score_15\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = 50 # Choose a sample to visualize\n",
    "pca = pca_on_data(5, data)\n",
    "plt.plot(data[t])\n",
    "plt.plot(pca.inverse_transform(pca.transform(data[[t]])).ravel())\n",
    "plt.title(\"DEB1 - A reconstructed sample (A1-2015)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = 50 # Choose a sample to visualize\n",
    "pca = pca_on_data(15, data)\n",
    "plt.plot(data[t])\n",
    "plt.plot(pca.inverse_transform(pca.transform(data[[t]])).ravel())\n",
    "plt.title(\"DEB1 - A reconstructed sample (A1-2015)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,figsize=(10,6),sharex=True)\n",
    "df.plot(ax = axes[0],color='b',title=\"DEB1\")\n",
    "(dataframe/30).plot(ax = axes[1],color=['k','r'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nb : \n",
    "- PCA is sensible to outliers\n",
    "- PCA gives an indicator close to the variance\n",
    "- PCA is better than variance in the sense that it can take multiple variables and learn from it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating train / test datasets\n",
    "\n",
    "**Required parameters**\n",
    "\n",
    "- A resampling_scale for optional resampling.\n",
    "- A length for the intervals. l_intervals.\n",
    "- The stride between the intervals. Default to l_intervals. Min to 1.\n",
    "- The proportion of test/train to evaluate if we don't overfit.\n",
    "- A scale for fitting the data between 0 and 1, or normalizing it. \n",
    "\n",
    "\n",
    "**Required information to analyse performance**\n",
    "\n",
    "- The method should return train and test, as well as all the corresponding timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def generate_train_test(obs_dict, target, scale, l_intervals, stride, test_size=0.):\n",
    "    l = l_intervals // 2\n",
    "    \n",
    "    timestamps = pd.DatetimeIndex([])\n",
    "    reactor_sites_array = []\n",
    "    data = []\n",
    "    \n",
    "    for reactor_site, obs in obs_dict.items():\n",
    "        list_df_resampled = obs.low_regime_intervals.split_between(scale.scale(obs.full_concatenated_df[target]),time=timedelta(days=3))\n",
    "        for df in list_df_resampled:        \n",
    "            indices = np.arange(l,df.shape[0]-l, stride)\n",
    "            timestamps = timestamps.append(df.index[indices])\n",
    "            data += [df.iloc[i-l:i+l].values[np.newaxis,:,:] for i in indices]\n",
    "            reactor_sites_array+=[np.repeat(reactor_site,len(indices))]\n",
    "    \n",
    "    reactor_sites_array = np.concatenate(reactor_sites_array,axis=0)\n",
    "    data = np.concatenate(data, axis=0)  \n",
    "    \n",
    "    not_nan_idx = ~np.isnan(data).any(axis=1).any(axis=1)\n",
    "    data = data[not_nan_idx]\n",
    "    timestamps = timestamps[not_nan_idx]\n",
    "    reactor_sites_array = reactor_sites_array[not_nan_idx]\n",
    "    \n",
    "    data = (((data - data.min())/(data.max() - data.min()))*2 - 1)\n",
    "    #train, test, train_timestamps, test_timestamps = train_test_split(data, timestamps)\n",
    "    return data, timestamps, reactor_sites_array#train, test, timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = [deb1[0],deb1[1]]\n",
    "data, timestamps,reactor_sites_array = generate_train_test(obs_dict, target, HourScale(12), 50, 5, test_size=0.)\n",
    "print(data.shape)\n",
    "print(\"The reactor corresponding :\",reactor_sites_array.shape)\n",
    "print(\"The timestamps corresponding :\",timestamps.shape)\n",
    "x_train = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a simple neural network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "from keras.models import Model\n",
    "\n",
    "x = Input(shape=x_train.shape[1:])\n",
    "\n",
    "h = Flatten()(x)\n",
    "h = Dense(64, activation='elu')(h)\n",
    "h = Dense(16, use_bias=False)(h)\n",
    "h = Dense(64, activation='elu')(h)\n",
    "\n",
    "x_recons = Dense(x_train.shape[2]*x_train.shape[1])(h)\n",
    "x_recons = Reshape(x_train.shape[1:])(x_recons)\n",
    "\n",
    "mlt = Model(x, x_recons)\n",
    "mlt.compile('adam', 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 256\n",
    "\n",
    "mlt = model_from_train(x_train)\n",
    "mlt.fit(x_train, x_train,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train_reconstructed = mlt.predict(x_train)\n",
    "print(x_train_reconstructed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,figsize=(5,6),sharex=True)\n",
    "i = 2\n",
    "axes[0].plot(x_train[i])\n",
    "axes[1].plot(x_train_reconstructed[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = np.sum(np.sum((x_train_reconstructed - x_train)**2,axis = 1),axis=1) # All features combined\n",
    "score_index = score.argsort()[::-1]\n",
    "results = list(zip(timestamps[score_index],score[score_index],reactor_sites_array[score_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "j = 40\n",
    "timestamp, s, reactor_site = results[j]\n",
    "print(s)\n",
    "print(timestamp)\n",
    "print(reactor_site)\n",
    "fig, axes = plt.subplots(2,figsize=(10,6))\n",
    "HourScale(12).scale(obs_dict[reactor_site].full_concatenated_df[target])[timestamp-timedelta(days=12):timestamp+timedelta(days=12)].plot(ax=axes[0])\n",
    "axes[1].plot(x_train_reconstructed[score_index[j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target = [deb1[0],deb1[1],pre[0],pui[0]]\n",
    "data, timestamps_,reactor_sites_array = generate_train_test(obs_dict, target, HourScale(12), 50, 5, test_size=0.)\n",
    "print(data.shape)\n",
    "print(\"The reactor corresponding :\",reactor_sites_array.shape)\n",
    "print(\"The timestamps corresponding :\",timestamps.shape)\n",
    "x_train_ = data\n",
    "epochs = 300\n",
    "batch_size = 256\n",
    "\n",
    "x = Input(shape=x_train_.shape[1:])\n",
    "\n",
    "h = Flatten()(x)\n",
    "h = Dense(128, activation='elu')(h)\n",
    "h = Dense(64, use_bias=False)(h)\n",
    "h = Dense(128, activation='elu')(h)\n",
    "\n",
    "x_recons = Dense(x_train_.shape[2]*x_train_.shape[1])(h)\n",
    "x_recons = Reshape(x_train_.shape[1:])(x_recons)\n",
    "\n",
    "mlt = Model(x, x_recons)\n",
    "mlt.compile('adam', 'mse')\n",
    "\n",
    "mlt.fit(x_train_, x_train_,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=2\n",
    ")\n",
    "x_train_reconstructed_ = mlt.predict(x_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_ = np.sum(np.sum((x_train_reconstructed_ - x_train_)**2,axis = 1),axis=1) # All features combined\n",
    "score_index_ = score_.argsort()[::-1]\n",
    "results_ = list(zip(timestamps_[score_index_],score_[score_index_],reactor_sites_array[score_index_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "j = 5\n",
    "timestamp, s, reactor_site = results_[j]\n",
    "print(s)\n",
    "print(timestamp)\n",
    "print(reactor_site)\n",
    "\n",
    "fig, axes = plt.subplots(2,3,figsize=(12,6),sharex=True)\n",
    "i = score_index_[j]\n",
    "axes[0,0].plot(x_train_[i,:,:2])\n",
    "axes[0,1].plot(x_train_[i,:,2],color='k')\n",
    "axes[0,2].plot(x_train_[i,:,3],color='g')\n",
    "axes[1,0].plot(x_train_reconstructed_[i,:,:2])\n",
    "axes[1,1].plot(x_train_reconstructed_[i,:,2],color='k')\n",
    "axes[1,2].plot(x_train_reconstructed_[i,:,3],color='g')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learning better anomaly classifiers\n",
    "\n",
    "Using neural networks to automatically learn our discriminative functions.\n",
    "\n",
    "After this first train, these networks can be refined in a supervised fashion.\n",
    "\n",
    "#### Exploring if the step function can be learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obs = obs_dict[\"A1\"]\n",
    "rolling_size=36\n",
    "half_window = int(rolling_size / 2)-1\n",
    "data = [] #data = np.concatenate(([[]for i in range(3)],[[1],[2],[3]]),axis=0)\n",
    "for df in obs.low_regime_intervals.split_between(MinutesScale().scale(obs.full_concatenated_df), time=timedelta(days=3)):\n",
    "    df = df[deb1[0]]\n",
    "    data+= [np.array([df.values[t - half_window:t + half_window] for t in range(half_window, len(df) - half_window)])]\n",
    "data = np.concatenate((data),axis=0)\n",
    "print(data.shape)\n",
    "#features_df.loc[\"step\",\"6h\"].df.dropna().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input,Concatenate, MaxPooling1D,Conv1D,UpSampling1D,Input, Dense, Lambda, Layer, Dropout, Conv1D, BatchNormalization, Activation, Reshape,Flatten\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras import optimizers\n",
    "\n",
    "x = Input(shape=x_train.shape[1:])\n",
    "\n",
    "# Neural Network parameters\n",
    "intermediate_dim = 256\n",
    "latent_dim = 10 \n",
    "\n",
    "# Encoder\n",
    "h = x\n",
    "h = Dense(intermediate_dim, activation='relu')(h)\n",
    "h = Dense(intermediate_dim//2, activation='relu')(h)\n",
    "y = h\n",
    "step_learner = Model(x,y)\n",
    "step_learner.compile(optimizer='rmsprop', loss='mean_squared_error')\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 256\n",
    "\n",
    "vae.fit(x_train, score_train,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test,score_test),\n",
    "        verbose=0\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tsc]",
   "language": "python",
   "name": "conda-env-tsc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
