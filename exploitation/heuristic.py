import numpy as np
from exploitation.trie import Node
from scipy.stats import norm

def to_symbols(breakpoints, value_symbols):
    symbols = []
    _a = len(breakpoints) # number of classes -1 = a-1
    for value in value_symbols:
        if value > breakpoints[-1]:
            symbols+=[_a]
        else:
            symbols+=[np.argmax(breakpoints > value)]
    return np.array(symbols)

def word_to_str(word):
    return ''.join([str(i) for i in word])

def complete_with_random_indices(first_indices, n_indices):
    next_random_indices = np.delete(np.random.permutation(n_indices), first_indices)
    return np.concatenate((first_indices,next_random_indices))

def inner_outer_heuristics(series, w_length, a, w):
    """
    :param w_length: pattern size
    :param a: Size of alphabet (number of different symbols)
    :param w: Word Length
    """
    ts_length = len(series)
    n = w_length # paper notation
    assert w_length%w == 0, "w should be multiple of w_length"
    distribution_estimate = norm(loc=np.mean(series),scale = np.std(series))
    breakpoints = distribution_estimate.ppf(np.linspace(0,1,a+1)[1:-1])
    trie_word = Node(w)
    word_list = []
    for i in range(ts_length - n):
        pattern = series[i:i+w_length]
        mean_value_symbols = np.mean(pattern.reshape(w_length//w, -1,order="F"),axis=0)
        word = to_symbols(breakpoints,mean_value_symbols)
        str_word = word_to_str(word)
        word_list.append(str_word)
        trie_word.add_word_index(str_word,i)
    f_list = np.array([len(trie_word.get_word_indices(word)) for word in word_list])
    first_indices = np.where(f_list.min()==f_list)[0]
    outer_heuristic_indices = complete_with_random_indices(first_indices, ts_length - n)
    return outer_heuristic_indices, word_list, trie_word

